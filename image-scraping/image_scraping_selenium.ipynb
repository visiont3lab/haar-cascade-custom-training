{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image-scraping-selenium.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpq3VfE1DJtv"
      },
      "source": [
        "## Get a dataset: Scraping using Selenium\n",
        "\n",
        "* We are going to use selenium library to retrieve google images associated to a specific query.\n",
        "* Another interesting tool for collecting dataset is [**Google Dataset Search**](https://datasetsearch.research.google.com/)\n",
        "* Label data using [**Online Labeling Tool (MakeSense)**](https://www.makesense.ai/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvxeaeF9DBvG"
      },
      "source": [
        "# Setup selenium colab: https://stackoverflow.com/questions/51046454/how-can-we-use-selenium-webdriver-in-colab-research-google-com\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qgi5AlDC3op"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from PIL import Image\n",
        "import io\n",
        "import hashlib\n",
        "\n",
        "# Images are going to be downloaded inside downloads folder\n",
        "download_folder = \"downloads\"\n",
        "if not os.path.exists(download_folder):\n",
        "    os.makedirs(download_folder)\n",
        "\n",
        "#----------------------\n",
        "#change your set of queries here\n",
        "queries = [\"smartphone telephone\"]  \n",
        "#----------------------\n",
        "\n",
        "# All in same directory\n",
        "#DRIVER_PATH = 'chromedriver.exe'\n",
        "\n",
        "def fetch_image_urls(query:str, max_links_to_fetch:int, wd:webdriver, sleep_between_interactions:int=1):\n",
        "    def scroll_to_end(wd):\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(sleep_between_interactions)        \n",
        "    \n",
        "    # build the google query\n",
        "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
        "\n",
        "    # load the page\n",
        "    wd.get(search_url.format(q=query))\n",
        "\n",
        "    image_urls = set()\n",
        "    image_count = 0\n",
        "    results_start = 0\n",
        "    error_clicks = 0\n",
        "    while (image_count < max_links_to_fetch) & (error_clicks < 30): # error clicks to stop when there are no more results to show by Google Images. You can tune the number\n",
        "        scroll_to_end(wd)\n",
        "\n",
        "        print('Starting search for Images')\n",
        "\n",
        "        # get all image thumbnail results\n",
        "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
        "        number_results = len(thumbnail_results)\n",
        "        \n",
        "        print(f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\")\n",
        "        for img in thumbnail_results[results_start:max_links_to_fetch]:\n",
        "            # try to click every thumbnail such that we can get the real image behind it\n",
        "            print(\"Total Errors till now:\", error_clicks)\n",
        "            try:\n",
        "                print('Trying to Click the Image')\n",
        "                img.click()\n",
        "                time.sleep(sleep_between_interactions)\n",
        "                print('Image Click Successful!')\n",
        "            except Exception:\n",
        "                error_clicks = error_clicks + 1\n",
        "                print('ERROR: Unable to Click the Image')\n",
        "                if(results_start < number_results):\n",
        "                \tcontinue\n",
        "                else:\n",
        "                \tbreak\n",
        "                \t\n",
        "            results_start = results_start + 1\n",
        "\n",
        "            # extract image urls    \n",
        "            print('Extracting of Image URLs')\n",
        "            actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
        "            for actual_image in actual_images:\n",
        "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
        "                    image_urls.add(actual_image.get_attribute('src'))\n",
        "\n",
        "            image_count = len(image_urls)\n",
        "\n",
        "            print('Current Total Image Count:', image_count)\n",
        "\n",
        "            if len(image_urls) >= max_links_to_fetch:\n",
        "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
        "                break\n",
        "            else:\n",
        "                load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
        "                if load_more_button:\n",
        "                    wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
        "            \t        \n",
        "        results_start = len(thumbnail_results)\n",
        "\n",
        "    return image_urls\n",
        "\n",
        "def persist_image(folder_path:str,file_name:str,url:str):\n",
        "    try:\n",
        "        image_content = requests.get(url).content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not download {url} - {e}\")\n",
        "\n",
        "    try:\n",
        "        image_file = io.BytesIO(image_content)\n",
        "        image = Image.open(image_file).convert('RGB')\n",
        "        folder_path = os.path.join(folder_path,file_name)\n",
        "        if os.path.exists(folder_path):\n",
        "            file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
        "        else:\n",
        "            os.mkdir(folder_path)\n",
        "            file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
        "        with open(file_path, 'wb') as f:\n",
        "            image.save(f, \"JPEG\", quality=85)\n",
        "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not save {url} - {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "  for query in queries:\n",
        "      wd.get('https://google.com')\n",
        "      search_box = wd.find_element_by_css_selector('input.gLFyf')\n",
        "      search_box.send_keys(query)\n",
        "      links = fetch_image_urls(query,20,wd) # 500 denotes no. of images you want to download\n",
        "      images_path = download_folder\n",
        "      for i in links:\n",
        "          persist_image(images_path,query,i)\n",
        "  wd.quit()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}